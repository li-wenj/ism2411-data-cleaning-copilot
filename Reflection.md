# Reflection

For this project, I used GitHub Copilot mainly to help generate the helper functions in `data_cleaning.py`. I started by writing short comments above each function, such as “This function loads the raw CSV into a DataFrame so we can clean it” and “This function standardizes column names and renames prodname/qty.” Then I typed part of a line (for example, `df =`) and let Copilot suggest the rest. It helped me quickly write the first versions of `load_data`, `clean_column_names`, `handle_missing_values`, and `remove_invalid_rows`, as well as some of the basic structure of the `if __name__ == "__main__"` block.

After accepting Copilot’s suggestions, I made several changes so the code matched the assignment and the actual dataset. For example, Copilot initially suggested generic column renaming, but I updated the logic to specifically rename `prodname` to `product_name` and `qty` to `quantity`, which are the real column names in `sales_data_raw.csv`. I also adjusted the missing-value handling to drop rows only when `price` or `quantity` was missing, and I added checks to remove rows with negative values in those columns. Finally, I added a separate `clean_text_columns` function to strip spaces from `product_name` and `category`, because that was a specific requirement in the instructions.

Through this process, I learned how to use pandas to clean a messy CSV in a more systematic way. I saw how important it is to standardize column names, convert numeric fields properly, and be explicit about how to handle missing and invalid data. I also learned that Copilot is very helpful for getting started quickly, but it does not “know” the assignment requirements or the exact shape of my data. I still had to read the prompts carefully, inspect the raw file, and edit Copilot’s code so it matched what my professor wanted. Overall, Copilot felt like a useful assistant for boilerplate and ideas, but I had to stay in control of the final design and logic.
